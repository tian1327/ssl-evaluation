Using l_train + val + unlabeled_in_oracle for training!!
######################################################################################################################################################
Creating Dataloaders
Reading split file from data/semi_aves/l_train_val_utrain_in_oracle.txt
All good for split l_train_val_utrain_in_oracle
# images in l_train_val_utrain_in_oracle: 32599
Reading split file from data/semi_aves/u_train_in.txt
All good for split u_train_in
# images in u_train_in: 26640
Reading split file from data/semi_aves/test.txt
All good for split test
# images in test: 8000
Reading split file from data/semi_aves/test.txt
All good for split test
# images in test: 8000


labeled data : 32599, unlabeled data : 26640
validation data : 8000, test data : 8000
#classes : 200
Dataloaders created successfully!
######################################################################################################################################################
=> loading checkpoint '/scratch/group/real-fs/model_ckpts/inat_resnet50.pth.tar'
=> loaded model from '/scratch/group/real-fs/model_ckpts/inat_resnet50.pth.tar'
using #GPUs: 1
parameters :  Namespace(MoCo=False, alg='supervised', alpha=0.1, batch_size=32, consis_coef=1.0, continue_training=False, em=0, exp_dir='semi_aves_supervised_in_inat_oracle', exp_prefix='results', init='inat', input_size=224, kd_T=1.0, load_dir='/scratch/group/real-fs/model_ckpts/inat_resnet50.pth.tar', lr=0.001, model='resnet50', num_iter=15000, num_workers=12, path_t='', print_freq=100, retrieval_split=None, root='data', task='semi_aves', threshold=0.95, trainval=False, trainval_un_in_oracle=True, unlabel='in', val_freq=200, warmup=1000, wd=0.0001)
train | Iteration 100/15000 | Loss 4.709824 | Top1 Acc 10.00% | Top5 Acc 23.56%
train | Iteration 200/15000 | Loss 3.436998 | Top1 Acc 33.03% | Top5 Acc 56.81%
Val | Iteration 200/15000 | Loss 3.059202 | Top1 Acc 37.56% | Top5 Acc 60.94%
train | Iteration 300/15000 | Loss 2.733559 | Top1 Acc 42.28% | Top5 Acc 68.59%
train | Iteration 400/15000 | Loss 2.399920 | Top1 Acc 47.19% | Top5 Acc 72.97%
Val | Iteration 400/15000 | Loss 2.104796 | Top1 Acc 49.98% | Top5 Acc 75.34%
train | Iteration 500/15000 | Loss 2.202057 | Top1 Acc 50.62% | Top5 Acc 74.28%
train | Iteration 600/15000 | Loss 2.021090 | Top1 Acc 53.31% | Top5 Acc 77.22%
Val | Iteration 600/15000 | Loss 1.768864 | Top1 Acc 55.71% | Top5 Acc 81.09%
train | Iteration 700/15000 | Loss 1.869843 | Top1 Acc 56.34% | Top5 Acc 79.59%
train | Iteration 800/15000 | Loss 1.858087 | Top1 Acc 56.25% | Top5 Acc 79.06%
Val | Iteration 800/15000 | Loss 1.510613 | Top1 Acc 61.27% | Top5 Acc 85.40%
train | Iteration 900/15000 | Loss 1.733070 | Top1 Acc 57.69% | Top5 Acc 81.09%
train | Iteration 1000/15000 | Loss 1.744411 | Top1 Acc 58.50% | Top5 Acc 80.56%
Val | Iteration 1000/15000 | Loss 1.361612 | Top1 Acc 63.24% | Top5 Acc 87.58%
train | Iteration 1100/15000 | Loss 1.605447 | Top1 Acc 61.38% | Top5 Acc 82.50%
train | Iteration 1200/15000 | Loss 1.598034 | Top1 Acc 61.22% | Top5 Acc 82.19%
Val | Iteration 1200/15000 | Loss 1.308069 | Top1 Acc 65.29% | Top5 Acc 88.03%
train | Iteration 1300/15000 | Loss 1.534927 | Top1 Acc 62.78% | Top5 Acc 82.91%
train | Iteration 1400/15000 | Loss 1.595940 | Top1 Acc 60.34% | Top5 Acc 82.34%
Val | Iteration 1400/15000 | Loss 1.273950 | Top1 Acc 66.14% | Top5 Acc 88.25%
train | Iteration 1500/15000 | Loss 1.544403 | Top1 Acc 62.03% | Top5 Acc 83.38%
train | Iteration 1600/15000 | Loss 1.541267 | Top1 Acc 62.16% | Top5 Acc 82.91%
Val | Iteration 1600/15000 | Loss 1.219326 | Top1 Acc 67.04% | Top5 Acc 89.03%
train | Iteration 1700/15000 | Loss 1.508425 | Top1 Acc 62.50% | Top5 Acc 83.19%
train | Iteration 1800/15000 | Loss 1.506192 | Top1 Acc 63.81% | Top5 Acc 82.59%
Val | Iteration 1800/15000 | Loss 1.203124 | Top1 Acc 66.95% | Top5 Acc 89.21%
train | Iteration 1900/15000 | Loss 1.485917 | Top1 Acc 63.16% | Top5 Acc 83.19%
train | Iteration 2000/15000 | Loss 1.495681 | Top1 Acc 62.34% | Top5 Acc 83.16%
Val | Iteration 2000/15000 | Loss 1.146425 | Top1 Acc 69.08% | Top5 Acc 89.34%
train | Iteration 2100/15000 | Loss 1.411475 | Top1 Acc 65.41% | Top5 Acc 84.03%
train | Iteration 2200/15000 | Loss 1.392233 | Top1 Acc 65.47% | Top5 Acc 84.41%
Val | Iteration 2200/15000 | Loss 1.151254 | Top1 Acc 69.16% | Top5 Acc 89.31%
train | Iteration 2300/15000 | Loss 1.401574 | Top1 Acc 65.66% | Top5 Acc 84.53%
train | Iteration 2400/15000 | Loss 1.377064 | Top1 Acc 65.50% | Top5 Acc 83.88%
Val | Iteration 2400/15000 | Loss 1.115751 | Top1 Acc 69.92% | Top5 Acc 89.70%
train | Iteration 2500/15000 | Loss 1.385307 | Top1 Acc 65.41% | Top5 Acc 84.59%
train | Iteration 2600/15000 | Loss 1.332427 | Top1 Acc 66.78% | Top5 Acc 85.38%
Val | Iteration 2600/15000 | Loss 1.092636 | Top1 Acc 70.25% | Top5 Acc 90.04%
train | Iteration 2700/15000 | Loss 1.359214 | Top1 Acc 65.91% | Top5 Acc 84.66%
train | Iteration 2800/15000 | Loss 1.355619 | Top1 Acc 66.16% | Top5 Acc 85.22%
Val | Iteration 2800/15000 | Loss 1.111013 | Top1 Acc 70.49% | Top5 Acc 89.49%
train | Iteration 2900/15000 | Loss 1.353429 | Top1 Acc 66.09% | Top5 Acc 85.56%
train | Iteration 3000/15000 | Loss 1.348047 | Top1 Acc 65.81% | Top5 Acc 85.12%
Val | Iteration 3000/15000 | Loss 1.055673 | Top1 Acc 71.33% | Top5 Acc 90.12%
train | Iteration 3100/15000 | Loss 1.306224 | Top1 Acc 66.78% | Top5 Acc 85.28%
train | Iteration 3200/15000 | Loss 1.234171 | Top1 Acc 68.84% | Top5 Acc 86.75%
Val | Iteration 3200/15000 | Loss 1.079121 | Top1 Acc 71.36% | Top5 Acc 89.97%
train | Iteration 3300/15000 | Loss 1.275987 | Top1 Acc 67.38% | Top5 Acc 85.81%
train | Iteration 3400/15000 | Loss 1.262673 | Top1 Acc 68.91% | Top5 Acc 85.28%
Val | Iteration 3400/15000 | Loss 1.064215 | Top1 Acc 70.99% | Top5 Acc 90.04%
train | Iteration 3500/15000 | Loss 1.213280 | Top1 Acc 69.69% | Top5 Acc 87.25%
train | Iteration 3600/15000 | Loss 1.230324 | Top1 Acc 68.72% | Top5 Acc 86.84%
Val | Iteration 3600/15000 | Loss 1.048514 | Top1 Acc 71.61% | Top5 Acc 90.28%
train | Iteration 3700/15000 | Loss 1.275086 | Top1 Acc 67.44% | Top5 Acc 86.50%
train | Iteration 3800/15000 | Loss 1.277943 | Top1 Acc 68.41% | Top5 Acc 86.03%
Val | Iteration 3800/15000 | Loss 1.060285 | Top1 Acc 71.45% | Top5 Acc 90.16%
train | Iteration 3900/15000 | Loss 1.268733 | Top1 Acc 67.50% | Top5 Acc 86.19%
train | Iteration 4000/15000 | Loss 1.306675 | Top1 Acc 67.31% | Top5 Acc 85.28%
Val | Iteration 4000/15000 | Loss 1.013892 | Top1 Acc 73.15% | Top5 Acc 90.64%
train | Iteration 4100/15000 | Loss 1.206638 | Top1 Acc 69.44% | Top5 Acc 86.94%
train | Iteration 4200/15000 | Loss 1.211683 | Top1 Acc 69.44% | Top5 Acc 86.47%
Val | Iteration 4200/15000 | Loss 1.026764 | Top1 Acc 72.78% | Top5 Acc 90.41%
train | Iteration 4300/15000 | Loss 1.184332 | Top1 Acc 70.47% | Top5 Acc 87.31%
train | Iteration 4400/15000 | Loss 1.117536 | Top1 Acc 71.34% | Top5 Acc 88.78%
Val | Iteration 4400/15000 | Loss 1.033493 | Top1 Acc 72.45% | Top5 Acc 90.20%
train | Iteration 4500/15000 | Loss 1.179948 | Top1 Acc 71.38% | Top5 Acc 86.44%
train | Iteration 4600/15000 | Loss 1.170179 | Top1 Acc 70.91% | Top5 Acc 87.00%
Val | Iteration 4600/15000 | Loss 1.006258 | Top1 Acc 72.96% | Top5 Acc 90.44%
train | Iteration 4700/15000 | Loss 1.219709 | Top1 Acc 69.00% | Top5 Acc 86.47%
train | Iteration 4800/15000 | Loss 1.181345 | Top1 Acc 69.41% | Top5 Acc 87.03%
Val | Iteration 4800/15000 | Loss 0.999828 | Top1 Acc 72.75% | Top5 Acc 90.60%
train | Iteration 4900/15000 | Loss 1.188874 | Top1 Acc 69.41% | Top5 Acc 86.59%
train | Iteration 5000/15000 | Loss 1.178882 | Top1 Acc 70.12% | Top5 Acc 86.50%
Val | Iteration 5000/15000 | Loss 1.009516 | Top1 Acc 72.81% | Top5 Acc 90.58%
train | Iteration 5100/15000 | Loss 1.207701 | Top1 Acc 69.19% | Top5 Acc 86.69%
train | Iteration 5200/15000 | Loss 1.114084 | Top1 Acc 72.16% | Top5 Acc 88.62%
Val | Iteration 5200/15000 | Loss 0.998911 | Top1 Acc 73.30% | Top5 Acc 90.75%
train | Iteration 5300/15000 | Loss 1.183182 | Top1 Acc 70.00% | Top5 Acc 86.91%
train | Iteration 5400/15000 | Loss 1.102073 | Top1 Acc 72.47% | Top5 Acc 87.84%
Val | Iteration 5400/15000 | Loss 1.012092 | Top1 Acc 72.78% | Top5 Acc 90.61%
train | Iteration 5500/15000 | Loss 1.119012 | Top1 Acc 71.50% | Top5 Acc 88.12%
train | Iteration 5600/15000 | Loss 1.120065 | Top1 Acc 71.69% | Top5 Acc 87.72%
Val | Iteration 5600/15000 | Loss 0.987753 | Top1 Acc 73.67% | Top5 Acc 90.67%
train | Iteration 5700/15000 | Loss 1.125802 | Top1 Acc 71.34% | Top5 Acc 87.28%
train | Iteration 5800/15000 | Loss 1.089747 | Top1 Acc 72.72% | Top5 Acc 88.34%
Val | Iteration 5800/15000 | Loss 0.995180 | Top1 Acc 73.36% | Top5 Acc 90.75%
train | Iteration 5900/15000 | Loss 1.151118 | Top1 Acc 71.22% | Top5 Acc 87.19%
train | Iteration 6000/15000 | Loss 1.163132 | Top1 Acc 70.53% | Top5 Acc 86.47%
Val | Iteration 6000/15000 | Loss 0.990406 | Top1 Acc 73.86% | Top5 Acc 90.76%
train | Iteration 6100/15000 | Loss 1.139307 | Top1 Acc 71.44% | Top5 Acc 87.44%
train | Iteration 6200/15000 | Loss 1.051378 | Top1 Acc 73.00% | Top5 Acc 89.34%
Val | Iteration 6200/15000 | Loss 0.978910 | Top1 Acc 73.58% | Top5 Acc 90.91%
train | Iteration 6300/15000 | Loss 1.060990 | Top1 Acc 72.69% | Top5 Acc 88.34%
train | Iteration 6400/15000 | Loss 1.058070 | Top1 Acc 73.59% | Top5 Acc 88.47%
Val | Iteration 6400/15000 | Loss 0.984659 | Top1 Acc 73.56% | Top5 Acc 90.75%
train | Iteration 6500/15000 | Loss 1.088734 | Top1 Acc 72.00% | Top5 Acc 88.00%
train | Iteration 6600/15000 | Loss 1.050488 | Top1 Acc 73.78% | Top5 Acc 88.84%
Val | Iteration 6600/15000 | Loss 0.983648 | Top1 Acc 73.71% | Top5 Acc 91.01%
train | Iteration 6700/15000 | Loss 1.121372 | Top1 Acc 72.28% | Top5 Acc 87.69%
train | Iteration 6800/15000 | Loss 1.036222 | Top1 Acc 73.56% | Top5 Acc 88.97%
Val | Iteration 6800/15000 | Loss 0.975448 | Top1 Acc 74.41% | Top5 Acc 91.16%
train | Iteration 6900/15000 | Loss 1.081824 | Top1 Acc 72.62% | Top5 Acc 88.12%
train | Iteration 7000/15000 | Loss 1.116538 | Top1 Acc 71.88% | Top5 Acc 87.44%
Val | Iteration 7000/15000 | Loss 0.959214 | Top1 Acc 74.39% | Top5 Acc 90.99%
train | Iteration 7100/15000 | Loss 1.090917 | Top1 Acc 72.78% | Top5 Acc 88.12%
train | Iteration 7200/15000 | Loss 1.056761 | Top1 Acc 73.34% | Top5 Acc 88.56%
Val | Iteration 7200/15000 | Loss 0.934424 | Top1 Acc 75.21% | Top5 Acc 91.46%
train | Iteration 7300/15000 | Loss 1.013965 | Top1 Acc 74.72% | Top5 Acc 89.03%
train | Iteration 7400/15000 | Loss 1.037986 | Top1 Acc 73.28% | Top5 Acc 88.97%
Val | Iteration 7400/15000 | Loss 0.953389 | Top1 Acc 74.72% | Top5 Acc 91.56%
train | Iteration 7500/15000 | Loss 1.041763 | Top1 Acc 74.19% | Top5 Acc 88.72%
train | Iteration 7600/15000 | Loss 1.012170 | Top1 Acc 74.47% | Top5 Acc 88.84%
Val | Iteration 7600/15000 | Loss 0.958373 | Top1 Acc 74.67% | Top5 Acc 91.24%
train | Iteration 7700/15000 | Loss 1.015646 | Top1 Acc 74.34% | Top5 Acc 88.50%
train | Iteration 7800/15000 | Loss 1.003549 | Top1 Acc 74.88% | Top5 Acc 89.66%
Val | Iteration 7800/15000 | Loss 0.946288 | Top1 Acc 74.95% | Top5 Acc 91.33%
train | Iteration 7900/15000 | Loss 1.023300 | Top1 Acc 73.97% | Top5 Acc 89.66%
train | Iteration 8000/15000 | Loss 1.056034 | Top1 Acc 73.47% | Top5 Acc 87.94%
Val | Iteration 8000/15000 | Loss 0.951141 | Top1 Acc 74.71% | Top5 Acc 91.58%
train | Iteration 8100/15000 | Loss 1.044246 | Top1 Acc 73.38% | Top5 Acc 88.03%
train | Iteration 8200/15000 | Loss 0.952880 | Top1 Acc 75.59% | Top5 Acc 90.34%
Val | Iteration 8200/15000 | Loss 0.949394 | Top1 Acc 74.79% | Top5 Acc 91.31%
train | Iteration 8300/15000 | Loss 1.009498 | Top1 Acc 74.25% | Top5 Acc 89.09%
train | Iteration 8400/15000 | Loss 0.978895 | Top1 Acc 75.78% | Top5 Acc 89.59%
Val | Iteration 8400/15000 | Loss 0.948625 | Top1 Acc 74.50% | Top5 Acc 91.61%
train | Iteration 8500/15000 | Loss 0.986012 | Top1 Acc 74.94% | Top5 Acc 89.41%
train | Iteration 8600/15000 | Loss 1.020987 | Top1 Acc 74.75% | Top5 Acc 88.56%
Val | Iteration 8600/15000 | Loss 0.927678 | Top1 Acc 75.71% | Top5 Acc 91.49%
train | Iteration 8700/15000 | Loss 1.010979 | Top1 Acc 75.50% | Top5 Acc 88.62%
train | Iteration 8800/15000 | Loss 0.980032 | Top1 Acc 75.75% | Top5 Acc 89.72%
Val | Iteration 8800/15000 | Loss 0.937937 | Top1 Acc 75.16% | Top5 Acc 91.34%
train | Iteration 8900/15000 | Loss 0.999209 | Top1 Acc 74.88% | Top5 Acc 89.56%
train | Iteration 9000/15000 | Loss 0.944134 | Top1 Acc 75.53% | Top5 Acc 90.19%
Val | Iteration 9000/15000 | Loss 0.918308 | Top1 Acc 75.24% | Top5 Acc 91.47%
train | Iteration 9100/15000 | Loss 0.967659 | Top1 Acc 75.81% | Top5 Acc 89.56%
train | Iteration 9200/15000 | Loss 0.973609 | Top1 Acc 75.62% | Top5 Acc 89.66%
Val | Iteration 9200/15000 | Loss 0.926029 | Top1 Acc 75.49% | Top5 Acc 91.53%
train | Iteration 9300/15000 | Loss 0.940034 | Top1 Acc 75.81% | Top5 Acc 90.47%
train | Iteration 9400/15000 | Loss 0.970458 | Top1 Acc 75.69% | Top5 Acc 89.22%
Val | Iteration 9400/15000 | Loss 0.928290 | Top1 Acc 75.85% | Top5 Acc 91.36%
train | Iteration 9500/15000 | Loss 0.928751 | Top1 Acc 75.78% | Top5 Acc 90.41%
train | Iteration 9600/15000 | Loss 0.984003 | Top1 Acc 74.97% | Top5 Acc 88.78%
Val | Iteration 9600/15000 | Loss 0.919690 | Top1 Acc 76.00% | Top5 Acc 91.62%
train | Iteration 9700/15000 | Loss 0.945067 | Top1 Acc 76.00% | Top5 Acc 90.22%
train | Iteration 9800/15000 | Loss 0.960086 | Top1 Acc 75.59% | Top5 Acc 90.25%
Val | Iteration 9800/15000 | Loss 0.929915 | Top1 Acc 75.49% | Top5 Acc 91.46%
train | Iteration 9900/15000 | Loss 0.947770 | Top1 Acc 76.69% | Top5 Acc 89.88%
train | Iteration 10000/15000 | Loss 0.921405 | Top1 Acc 76.59% | Top5 Acc 90.16%
Val | Iteration 10000/15000 | Loss 0.921592 | Top1 Acc 75.69% | Top5 Acc 91.64%
train | Iteration 10100/15000 | Loss 0.932539 | Top1 Acc 77.47% | Top5 Acc 90.53%
train | Iteration 10200/15000 | Loss 0.923549 | Top1 Acc 76.19% | Top5 Acc 90.16%
Val | Iteration 10200/15000 | Loss 0.915539 | Top1 Acc 76.01% | Top5 Acc 91.70%
train | Iteration 10300/15000 | Loss 0.903381 | Top1 Acc 77.66% | Top5 Acc 90.31%
train | Iteration 10400/15000 | Loss 0.891397 | Top1 Acc 77.66% | Top5 Acc 90.84%
Val | Iteration 10400/15000 | Loss 0.913293 | Top1 Acc 75.80% | Top5 Acc 91.70%
train | Iteration 10500/15000 | Loss 0.940804 | Top1 Acc 76.25% | Top5 Acc 89.47%
train | Iteration 10600/15000 | Loss 0.990234 | Top1 Acc 75.19% | Top5 Acc 88.72%
Val | Iteration 10600/15000 | Loss 0.914355 | Top1 Acc 75.95% | Top5 Acc 91.64%
train | Iteration 10700/15000 | Loss 0.916855 | Top1 Acc 77.38% | Top5 Acc 89.78%
train | Iteration 10800/15000 | Loss 0.927992 | Top1 Acc 77.03% | Top5 Acc 90.38%
Val | Iteration 10800/15000 | Loss 0.907393 | Top1 Acc 76.12% | Top5 Acc 91.80%
train | Iteration 10900/15000 | Loss 0.958068 | Top1 Acc 76.31% | Top5 Acc 89.41%
train | Iteration 11000/15000 | Loss 0.840540 | Top1 Acc 78.75% | Top5 Acc 91.59%
Val | Iteration 11000/15000 | Loss 0.910805 | Top1 Acc 76.22% | Top5 Acc 91.89%
train | Iteration 11100/15000 | Loss 0.952801 | Top1 Acc 75.62% | Top5 Acc 89.66%
train | Iteration 11200/15000 | Loss 0.878046 | Top1 Acc 77.97% | Top5 Acc 90.94%
Val | Iteration 11200/15000 | Loss 0.895050 | Top1 Acc 76.26% | Top5 Acc 91.78%
train | Iteration 11300/15000 | Loss 0.921074 | Top1 Acc 77.44% | Top5 Acc 89.69%
train | Iteration 11400/15000 | Loss 0.852704 | Top1 Acc 78.50% | Top5 Acc 91.44%
Val | Iteration 11400/15000 | Loss 0.901849 | Top1 Acc 76.70% | Top5 Acc 91.97%
train | Iteration 11500/15000 | Loss 0.921642 | Top1 Acc 76.62% | Top5 Acc 90.44%
train | Iteration 11600/15000 | Loss 0.911190 | Top1 Acc 77.62% | Top5 Acc 90.06%
Val | Iteration 11600/15000 | Loss 0.899972 | Top1 Acc 76.71% | Top5 Acc 91.76%
train | Iteration 11700/15000 | Loss 0.872797 | Top1 Acc 78.69% | Top5 Acc 90.69%
train | Iteration 11800/15000 | Loss 0.868615 | Top1 Acc 78.56% | Top5 Acc 90.66%
Val | Iteration 11800/15000 | Loss 0.893781 | Top1 Acc 76.56% | Top5 Acc 91.88%
train | Iteration 11900/15000 | Loss 0.901761 | Top1 Acc 77.81% | Top5 Acc 90.34%
train | Iteration 12000/15000 | Loss 0.963921 | Top1 Acc 76.03% | Top5 Acc 89.31%
Val | Iteration 12000/15000 | Loss 0.897658 | Top1 Acc 76.38% | Top5 Acc 91.91%
train | Iteration 12100/15000 | Loss 0.898612 | Top1 Acc 77.94% | Top5 Acc 90.25%
train | Iteration 12200/15000 | Loss 0.856873 | Top1 Acc 78.59% | Top5 Acc 91.00%
Val | Iteration 12200/15000 | Loss 0.896205 | Top1 Acc 76.56% | Top5 Acc 92.01%
train | Iteration 12300/15000 | Loss 0.864815 | Top1 Acc 78.38% | Top5 Acc 90.94%
train | Iteration 12400/15000 | Loss 0.932220 | Top1 Acc 76.84% | Top5 Acc 90.06%
Val | Iteration 12400/15000 | Loss 0.904059 | Top1 Acc 76.21% | Top5 Acc 91.94%
train | Iteration 12500/15000 | Loss 0.897204 | Top1 Acc 77.53% | Top5 Acc 90.03%
train | Iteration 12600/15000 | Loss 0.913391 | Top1 Acc 77.12% | Top5 Acc 90.38%
Val | Iteration 12600/15000 | Loss 0.894036 | Top1 Acc 76.56% | Top5 Acc 91.88%
train | Iteration 12700/15000 | Loss 0.885856 | Top1 Acc 78.56% | Top5 Acc 90.50%
train | Iteration 12800/15000 | Loss 0.893397 | Top1 Acc 78.00% | Top5 Acc 90.41%
Val | Iteration 12800/15000 | Loss 0.893447 | Top1 Acc 76.79% | Top5 Acc 92.08%
train | Iteration 12900/15000 | Loss 0.838144 | Top1 Acc 78.56% | Top5 Acc 90.69%
train | Iteration 13000/15000 | Loss 0.870315 | Top1 Acc 78.59% | Top5 Acc 90.97%
Val | Iteration 13000/15000 | Loss 0.893084 | Top1 Acc 76.60% | Top5 Acc 91.97%
train | Iteration 13100/15000 | Loss 0.862958 | Top1 Acc 78.84% | Top5 Acc 90.81%
train | Iteration 13200/15000 | Loss 0.857653 | Top1 Acc 78.69% | Top5 Acc 91.19%
Val | Iteration 13200/15000 | Loss 0.891383 | Top1 Acc 76.61% | Top5 Acc 91.86%
train | Iteration 13300/15000 | Loss 0.864327 | Top1 Acc 78.25% | Top5 Acc 90.66%
train | Iteration 13400/15000 | Loss 0.876536 | Top1 Acc 78.31% | Top5 Acc 90.56%
Val | Iteration 13400/15000 | Loss 0.898173 | Top1 Acc 76.79% | Top5 Acc 91.83%
train | Iteration 13500/15000 | Loss 0.900709 | Top1 Acc 77.47% | Top5 Acc 90.34%
train | Iteration 13600/15000 | Loss 0.873297 | Top1 Acc 78.69% | Top5 Acc 91.03%
Val | Iteration 13600/15000 | Loss 0.895741 | Top1 Acc 76.79% | Top5 Acc 91.88%
train | Iteration 13700/15000 | Loss 0.884632 | Top1 Acc 78.09% | Top5 Acc 90.25%
train | Iteration 13800/15000 | Loss 0.853571 | Top1 Acc 78.88% | Top5 Acc 91.06%
Val | Iteration 13800/15000 | Loss 0.895232 | Top1 Acc 76.66% | Top5 Acc 91.85%
train | Iteration 13900/15000 | Loss 0.862618 | Top1 Acc 79.12% | Top5 Acc 91.09%
train | Iteration 14000/15000 | Loss 0.847141 | Top1 Acc 79.09% | Top5 Acc 91.16%
Val | Iteration 14000/15000 | Loss 0.899043 | Top1 Acc 76.80% | Top5 Acc 91.83%
train | Iteration 14100/15000 | Loss 0.842423 | Top1 Acc 79.22% | Top5 Acc 91.22%
train | Iteration 14200/15000 | Loss 0.879953 | Top1 Acc 78.47% | Top5 Acc 90.59%
Val | Iteration 14200/15000 | Loss 0.891026 | Top1 Acc 76.84% | Top5 Acc 91.80%
train | Iteration 14300/15000 | Loss 0.882821 | Top1 Acc 78.59% | Top5 Acc 90.59%
train | Iteration 14400/15000 | Loss 0.880363 | Top1 Acc 77.69% | Top5 Acc 90.94%
Val | Iteration 14400/15000 | Loss 0.896953 | Top1 Acc 76.78% | Top5 Acc 91.88%
train | Iteration 14500/15000 | Loss 0.892927 | Top1 Acc 78.12% | Top5 Acc 89.84%
train | Iteration 14600/15000 | Loss 0.875273 | Top1 Acc 78.44% | Top5 Acc 90.31%
Val | Iteration 14600/15000 | Loss 0.892016 | Top1 Acc 76.66% | Top5 Acc 91.86%
train | Iteration 14700/15000 | Loss 0.881094 | Top1 Acc 77.84% | Top5 Acc 90.53%
train | Iteration 14800/15000 | Loss 0.927747 | Top1 Acc 76.12% | Top5 Acc 90.25%
Val | Iteration 14800/15000 | Loss 0.891484 | Top1 Acc 76.89% | Top5 Acc 91.97%
train | Iteration 14900/15000 | Loss 0.867167 | Top1 Acc 78.56% | Top5 Acc 90.69%
train | Iteration 15000/15000 | Loss 0.846428 | Top1 Acc 78.94% | Top5 Acc 90.94%
Val | Iteration 15000/15000 | Loss 0.892200 | Top1 Acc 76.70% | Top5 Acc 91.96%
Training complete in 28m 9s
Best val Acc: 76.89%
testLast Loss: 0.8940 Top1 Acc: 76.70% Top5 Acc: 92.01%
testBest Loss: 0.8915 Top1 Acc: 76.89% Top5 Acc: 91.97%
DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
